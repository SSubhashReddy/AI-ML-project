{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSubhashReddy/AI-ML-project/blob/main/Copy_of_Sample_ML_Submission_Template_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Local Food Wastage Management System\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -**S.Venkata Subhash Reddy\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Local Food Wastage Management System is designed to minimize food waste by creating an efficient channel for surplus food collection, redistribution, and disposal. It connects restaurants, hotels, households, supermarkets, and event organizers with NGOs, food banks, and needy communities to ensure edible surplus food reaches beneficiaries instead of ending up in landfills.\n",
        "\n",
        "The system operates through a centralized digital platform (web or mobile app) where donors can register surplus food details, including type, quantity, freshness, and pickup time. NGOs or collection agents are notified in real time, enabling quick allocation and transport to targeted locations. GPS integration helps track pickup and delivery, ensuring transparency and accountability. Donors can receive updates on the status of their contributions, and NGOs can manage their requests and delivery schedules effectively.\n",
        "\n",
        "To maintain hygiene and safety, the system follows standard food handling guidelines, ensuring the collected food is fit for consumption. Inedible food waste is diverted for composting or bioenergy production, promoting environmental sustainability.\n",
        "**Key features include:**\n",
        "\n",
        "User Registration & Authentication for donors, NGOs, and administrators.\n",
        "\n",
        "Food Donation Scheduling with automated matching based on location and urgency.\n",
        "\n",
        "Real-Time Tracking & Notifications to ensure timely collection and delivery.\n",
        "\n",
        "Analytics Dashboard for tracking total donations, beneficiaries served, and waste reduced.\n",
        "\n",
        "Waste-to-Energy/Compost Integration for inedible food management."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Food wastage is a significant issue, with many households and restaurants discarding surplus food while numerous people struggle with food insecurity. This project aims to develop a Local Food Wastage Management System, where:\n",
        "\n",
        "Restaurants and individuals can list surplus food.\n",
        "\n",
        "NGOs or individuals in need can claim the food.\n",
        "\n",
        "SQL stores available food details and locations.\n",
        "\n",
        "A Streamlit app enables interaction, filtering, CRUD operation and visualization.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "try:\n",
        "    # Use read_excel for .xlsx files\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(\"File loaded successfully!\")\n",
        "    print(df.head())  # Preview first few rows\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at {file_path}. Please check the path.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ],
      "metadata": {
        "id": "AHQe-WdKOJd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows = df.shape[0]\n",
        "num_columns = df.shape[1]\n",
        "\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing Values:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "File type & format → CSV, Excel, JSON, etc.\n",
        "\n",
        "Number of rows & columns → shape of the data.\n",
        "\n",
        "Column names & data types → numeric, text, date, etc.\n",
        "\n",
        "Missing values → how much data is incomplete.\n",
        "\n",
        "Basic statistics → min, max, mean, counts.\n",
        "\n",
        "Sample records → first few rows for a quick look."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "column_names = df.columns\n",
        "print(\"Column Names:\")\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable → column name\n",
        "\n",
        "DataType → numeric, object (text), datetime, etc.\n",
        "\n",
        "NonNullCount → how many non-missing values are there\n",
        "\n",
        "MissingCount → how many are missing\n",
        "\n",
        "UniqueValues → number of distinct values in the column"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "print(\"Unique Values for Each Variable:\")\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# 1. Load dataset (auto detect CSV/Excel)\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# 2. Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# 3. Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 4. Handle missing values\n",
        "#    - Numeric columns: fill with mean\n",
        "#    - Categorical columns: fill with mode\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['float64', 'int64']:\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "    else:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "# 5. Convert date columns automatically (if possible)\n",
        "for col in df.columns:\n",
        "    try:\n",
        "        df[col] = pd.to_datetime(df[col])\n",
        "    except (ValueError, TypeError):\n",
        "        pass  # Ignore if not a date\n",
        "\n",
        "# 6. Encode categorical variables (optional: for ML tasks)\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# 7. Final check\n",
        "print(\"Dataset is ready for analysis!\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Shape: e.g., “Dataset has 5,000 rows and 12 columns.”\n",
        "\n",
        "Top Contributors to Food Waste: e.g., “Restaurants account for 45% of surplus food donations.”\n",
        "\n",
        "Time Patterns: e.g., “Peak donations occur between 7 PM and 9 PM.”\n",
        "\n",
        "Food Type Trends: e.g., “Bakery items form the largest share (35%) of donations.”\n",
        "\n",
        "Waste Reduction Potential: e.g., “If redistribution improves by 15%, landfill waste could be cut by 2 tons/month.”\n",
        "\n",
        "Geographical Insights: e.g., “Zone A contributes more fresh produce, while Zone C has more packaged goods.”"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset (auto detect CSV/Excel)\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Example: Top 10 most donated food items\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(\n",
        "    data=df,\n",
        "    y='food_name',  # Corrected column name\n",
        "    order=df['food_name'].value_counts().head(10).index,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title(\"Top 10 Most Donated Food Items\", fontsize=16)\n",
        "plt.xlabel(\"Number of Donations\")\n",
        "plt.ylabel(\"Food Item\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal bar charts clearly display ranked categorical data, making it easy to compare donation counts across food items."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rice and Soup are the most donated items, while Fruits are the least among the top 10."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive — helps plan storage, logistics, and targeted donation drives for high-demand items.\n",
        "\n",
        "No negative growth — all items are receiving donations, just in varying amounts."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Ensure date column is in datetime format\n",
        "# Replace 'donation_date' with your actual date column name\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "# Drop rows without valid dates\n",
        "df = df.dropna(subset=['timestamp'])\n",
        "\n",
        "# Group by date and count donations\n",
        "daily_donations = df.groupby(df['timestamp'].dt.date).size()\n",
        "\n",
        "# Plot time-series\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(x=daily_donations.index, y=daily_donations.values, marker='o', color='orange')\n",
        "plt.title(\"Daily Food Donations Trend\", fontsize=16)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Donations\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show donation trends over time, highlighting peaks and dips clearly."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donations fluctuate significantly, with notable peaks around March, July, and October."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive — helps schedule campaigns during high-donation months.\n",
        "\n",
        "Negative — sharp dips suggest potential supply issues or donor disengagement during certain periods."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace 'donor_type' with your actual column name for donor categories\n",
        "donor_counts = df['type'].value_counts()\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(donor_counts, labels=donor_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title(\"Donations by Donor Category\", fontsize=16)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures the pie is a circle\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is ideal to compare proportional contributions from each donor category in a visually clear way."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supermarkets contribute the highest share of donations (26.2%).\n",
        "\n",
        "Grocery Stores (25.6%) and Restaurants (24.6%) follow closely.\n",
        "\n",
        "Catering Services contribute the least (23.6%), but still make up a significant share."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Highlights balanced contributions among donor categories, reducing over-reliance on a single source.\n",
        "\n",
        "Negative: Slightly lower contributions from Catering Services might indicate untapped potential for increasing supply."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace 'donation_datetime' with your actual datetime column name\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "# Drop rows without valid datetime\n",
        "df = df.dropna(subset=['timestamp'])\n",
        "\n",
        "# Extract weekday and hour\n",
        "df['weekday'] = df['timestamp'].dt.day_name()\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "\n",
        "# Create pivot table\n",
        "pivot_data = df.pivot_table(index='weekday', columns='hour', values='food_name', aggfunc='count')\n",
        "\n",
        "# Reorder weekdays\n",
        "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "pivot_data = pivot_data.reindex(weekday_order)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(pivot_data, cmap='YlOrRd', linewidths=0.5, annot=True, fmt='.0f')\n",
        "plt.title(\"Donations Heatmap by Weekday and Hour\", fontsize=16)\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Day of Week\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is the best choice for spotting patterns across two dimensions (weekday and hour) simultaneously. It helps identify peak donation times quickly."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest donation count (16) occurs on Thursday at 15:00.\n",
        "\n",
        "Other notable peaks: Monday at 9:00 & 10:00, Thursday at 3:00 and 8:00, Friday at 4:00, 15:00, and 21:00.\n",
        "\n",
        "Donations are generally lower in the early morning (2:00–6:00) and late night (21:00–23:00), except for Friday."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Knowing peak hours can help allocate volunteers and transportation more efficiently, reducing waste from delayed pickups.\n",
        "\n",
        "Negative: If storage is inadequate, high peak donations (like Thursday afternoons) could lead to spoilage before redistribution."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "print(\"First 5 rows:\\n\", df.head(), \"\\n\")\n",
        "print(\"Column names:\\n\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quickly spot relationships between numerical variables like Quantity, provider info, and meal details."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong link between provider and provider type.\n",
        "\n",
        "Certain meal/food types may have higher quantities.\n",
        "\n",
        "IDs have little predictive value."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Positive: Helps target donation drives and plan supply better.\n",
        "\n",
        "Negative: Over-reliance on few providers, risk of food wastage if expiry dates are close."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import pandas as pd\n",
        "import folium\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "print(\"This visualization cannot be generated as the dataset does not contain latitude and longitude columns.\")"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally chosen to show geographical distribution of food providers/receivers for location-based analysis."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not possible to generate since the dataset lacks latitude/longitude."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: If location data is added, mapping could optimize delivery routes and resource allocation.\n",
        "\n",
        "Negative: Current lack of location data limits ability to make location-based decisions, potentially causing inefficiencies."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "date_col = 'timestamp'\n",
        "category_col = 'type'\n",
        "\n",
        "# Ensure date is in datetime format\n",
        "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "# Drop rows without valid date or category\n",
        "df = df.dropna(subset=[date_col, category_col])\n",
        "\n",
        "# Create month-year column\n",
        "df['month_year'] = df[date_col].dt.to_period('M').astype(str)\n",
        "\n",
        "# Group and pivot data for stacked bar\n",
        "pivot_df = df.groupby(['month_year', category_col]).size().unstack(fill_value=0)\n",
        "\n",
        "# Plot stacked bar chart\n",
        "pivot_df.plot(kind='bar', stacked=True, figsize=(12,6))\n",
        "plt.title(\"Donations by Donor Type Over Time\", fontsize=16)\n",
        "plt.xlabel(\"Month-Year\")\n",
        "plt.ylabel(\"Number of Donations\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Donor Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart clearly shows the volume of donations over time while also comparing contributions by donor type in each month."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "March 2025 shows an extreme spike in donations across all donor types, especially catering services and grocery stores.\n",
        "\n",
        "Other months have relatively stable, lower donation levels."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifying donation peaks helps in planning storage, distribution, and manpower needs. The March spike may correspond to seasonal events or campaigns worth repeating.\n",
        "\n",
        "Negative: High dependency on irregular spikes could lead to inconsistent supply. If demand remains steady, months with low donations may risk shortages."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "donor_col = 'name'       # column containing donor names\n",
        "quantity_col = 'quantity'      # column containing donation quantity\n",
        "\n",
        "# Drop rows with missing donor or quantity\n",
        "df = df.dropna(subset=[donor_col, quantity_col])\n",
        "\n",
        "# Convert quantity to numeric if needed\n",
        "df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "\n",
        "# Group by donor and get top 10\n",
        "top_donors = df.groupby(donor_col)[quantity_col].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "top_donors.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title(\"Top 10 Donors by Quantity Donated\", fontsize=16)\n",
        "plt.xlabel(\"Donor Name\")\n",
        "plt.ylabel(\"Total Quantity\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is ideal for comparing quantities across a small number of categories—in this case, the top 10 donors. It clearly shows the ranking and scale of donations from each donor."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Williams PLC and Miller Ltd are the top two donors, with quantities significantly higher than the rest.\n",
        "\n",
        "The contribution gap between the top donors and the bottom donors (e.g., Bowman LLC) is notable."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifying key donors allows targeted engagement strategies, loyalty programs, and recognition campaigns to maintain their high contribution levels.\n",
        "\n",
        "Negative: Heavy reliance on a few top donors could pose a risk—if they stop contributing, total donations could drop significantly."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "food_col = 'food_name'   # column containing food item names\n",
        "quantity_col = 'quantity'  # column containing donation quantity\n",
        "\n",
        "# Drop missing values for these columns\n",
        "df = df.dropna(subset=[food_col, quantity_col])\n",
        "\n",
        "# Convert quantity to numeric if needed\n",
        "df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "\n",
        "# Group by food item and get top 10\n",
        "top_foods = df.groupby(food_col)[quantity_col].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "top_foods.plot(kind='bar', color='lightgreen', edgecolor='black')\n",
        "plt.title(\"Top 10 Food Items Donated\", fontsize=16)\n",
        "plt.xlabel(\"Food Item\")\n",
        "plt.ylabel(\"Total Quantity\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is effective for comparing donation quantities across different food categories. It clearly highlights which items are most frequently donated."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rice is the top donated item, followed by Soup and Dairy.\n",
        "\n",
        "Items like Fish and Fruits have lower donation quantities compared to staples like Rice and Bread."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: High donations of staple foods like Rice ensure basic food needs are met consistently.\n",
        "\n",
        "Negative: Lower donations of proteins (e.g., Fish, Chicken) and fresh produce (Fruits, Vegetables) may limit nutritional diversity for recipients, suggesting a need for targeted donation drives."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "date_col = 'timestamp'   # column with donation dates\n",
        "quantity_col = 'quantity'    # column with donation quantities\n",
        "\n",
        "# Drop missing values for these columns\n",
        "df = df.dropna(subset=[date_col, quantity_col])\n",
        "\n",
        "# Ensure date column is datetime\n",
        "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "# Convert quantity to numeric\n",
        "df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "\n",
        "# Remove rows with NaT in date\n",
        "df = df.dropna(subset=[date_col])\n",
        "\n",
        "# Group by date (daily totals)\n",
        "daily_donations = df.groupby(date_col)[quantity_col].sum()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(daily_donations.index, daily_donations.values, marker='o', linestyle='-', color='blue')\n",
        "plt.title(\"Quantity Donated Over Time\", fontsize=16)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Total Quantity\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A time series line plot is ideal for tracking donation quantity patterns over time, making it easy to spot trends, fluctuations, and seasonal spikes."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donations fluctuate significantly, with occasional large peaks reaching around 90–95 units.\n",
        "\n",
        "There is no consistent upward or downward trend, but periodic spikes suggest seasonal or event-driven donations."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Identifying peak donation periods can help schedule campaigns during high-engagement months to maximize collection.\n",
        "\n",
        "Negative: Inconsistent donations may create supply shortages in certain periods, affecting recipients; this suggests the need for targeted outreach in low-donation months."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "date_col = 'timestamp'   # column with donation dates\n",
        "quantity_col = 'quantity'    # column with donation quantities\n",
        "\n",
        "# Drop missing values\n",
        "df = df.dropna(subset=[date_col, quantity_col])\n",
        "\n",
        "# Ensure correct data types\n",
        "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "\n",
        "# Remove invalid rows\n",
        "df = df.dropna(subset=[date_col])\n",
        "\n",
        "# Extract day of the week (0=Monday, 6=Sunday)\n",
        "df['day_of_week'] = df[date_col].dt.day_name()\n",
        "\n",
        "# Group by day of week\n",
        "donations_by_day = df.groupby('day_of_week')[quantity_col].sum()\n",
        "\n",
        "# Reorder days\n",
        "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "donations_by_day = donations_by_day.reindex(days_order)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(donations_by_day.index, donations_by_day.values, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Donations by Day of the Week\", fontsize=16)\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Total Quantity Donated\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is perfect for comparing total donations across days of the week, making it easy to identify which days see more or less activity."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thursday is the highest donation day (~5200 units), followed by Monday and Friday.\n",
        "\n",
        "Saturday and Tuesday see the lowest donations (~2500–2600 units).\n",
        "\n",
        "Midweek (Wednesday) shows moderate donation activity."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: High activity days (Thursday, Monday, Friday) are great for targeted campaigns and events to maximize turnout.\n",
        "\n",
        "Negative: Low activity on weekends and Tuesdays may indicate disengagement—extra promotions or awareness drives could help balance donation distribution."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "date_col = 'timestamp'   # column with donation dates\n",
        "quantity_col = 'quantity'    # column with donation quantities\n",
        "\n",
        "# Drop missing values\n",
        "df = df.dropna(subset=[date_col, quantity_col])\n",
        "\n",
        "# Ensure correct data types\n",
        "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "\n",
        "# Remove invalid rows\n",
        "df = df.dropna(subset=[date_col])\n",
        "\n",
        "# Extract month name\n",
        "df['month'] = df[date_col].dt.month_name()\n",
        "\n",
        "# Group by month\n",
        "donations_by_month = df.groupby('month')[quantity_col].sum()\n",
        "\n",
        "# Order months in calendar order\n",
        "months_order = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "                \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "donations_by_month = donations_by_month.reindex(months_order)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(donations_by_month.index, donations_by_month.values, color='lightgreen', edgecolor='black')\n",
        "plt.title(\"Donations by Month\", fontsize=16)\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total Quantity Donated\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar chart clearly compares monthly donation totals, making it easy to spot seasonal peaks and low periods."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "March stands out massively with ~12,000 units—far higher than any other month.\n",
        "\n",
        "October shows the second-highest donations (~1,700 units).\n",
        "\n",
        "Most other months range between ~1,000–1,500 units.\n",
        "\n",
        "September sees the lowest donations (~900 units)."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: March could indicate a successful campaign or seasonal event—worth replicating or expanding.\n",
        "\n",
        "Negative: Months with consistently low donations (e.g., September, January, November) may require targeted outreach or special drives to boost contributions."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Replace with your actual column names\n",
        "donor_col = 'name'   # column with donor names\n",
        "quantity_col = 'quantity'  # column with donation quantities\n",
        "\n",
        "# Drop missing values\n",
        "df = df.dropna(subset=[donor_col, quantity_col])\n",
        "\n",
        "# Ensure quantity is numeric\n",
        "df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "df = df.dropna(subset=[quantity_col])\n",
        "\n",
        "# Group by donor and sum donations\n",
        "top_donors = df.groupby(donor_col)[quantity_col].sum().nlargest(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(top_donors.index[::-1], top_donors.values[::-1], color='skyblue', edgecolor='black')\n",
        "plt.title(\"Top 10 Donor Contributions\", fontsize=16)\n",
        "plt.xlabel(\"Total Quantity Donated\")\n",
        "plt.ylabel(\"Donor Name\")\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen because it clearly compares top donors’ contributions in descending order."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Williams PLC is the highest donor; top 3 donors contribute significantly more than the rest."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, insights can help target and retain high-value donors; no major negative growth seen, but dependency on few donors may be a risk."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Select only numeric columns for correlation\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Drop columns with all NaN values\n",
        "numeric_df = numeric_df.dropna(axis=1, how='all')\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen to visualize correlation between dataset variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most IDs are perfectly correlated, while Quantity shows very weak correlation with other variables."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Select only numeric columns for pairplot\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Drop columns with all NaN values\n",
        "numeric_df = numeric_df.dropna(axis=1, how='all')\n",
        "\n",
        "# Create Pair Plot\n",
        "sns.pairplot(numeric_df)\n",
        "plt.suptitle(\"Pair Plot of Numeric Features\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen to explore relationships and distributions between all numeric features simultaneously."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features mostly show no strong correlation except perfect diagonal self-correlations; distributions appear uniform."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H1: The average donation amount is greater than ₹500.\n",
        "\n",
        "H2: There is a significant difference in donation amounts between male and female donors.\n",
        "\n",
        "H3: Donation amount is correlated with donor age.\n",
        "\n",
        "For each, you would run:\n",
        "\n",
        "H1: One-sample t-test against μ = 500.\n",
        "\n",
        "H2: Independent t-test between male and female groups.\n",
        "\n",
        "H3: Pearson correlation test between DonationAmount and Age."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average donation > ₹500\n",
        "\n",
        "H₀: μ ≤ 500\n",
        "\n",
        "H₁: μ > 500\n",
        "\n",
        "Difference between male & female donation amounts\n",
        "\n",
        "H₀: μₘ = μₓ\n",
        "\n",
        "H₁: μₘ ≠ μₓ\n",
        "\n",
        "Correlation between donation amount & age\n",
        "\n",
        "H₀: ρ = 0\n",
        "\n",
        "H₁: ρ ≠ 0"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "\n",
        "# 1. One-sample t-test (Average donation > ₹500)\n",
        "# Assuming 'quantity' column exists and is numeric\n",
        "# Need to check if 'quantity' column exists before proceeding\n",
        "if 'quantity' in df.columns:\n",
        "    df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
        "    df = df.dropna(subset=['quantity']) # Drop rows with missing or invalid quantity\n",
        "\n",
        "    if not df['quantity'].empty:\n",
        "        t_stat1, p_val1 = stats.ttest_1samp(df['quantity'], 500)\n",
        "        p_val1_one_sided = p_val1 / 2 if t_stat1 > 0 else 1 - (p_val1 / 2)\n",
        "        print(f\"Test 1 (Mean > 500): t={t_stat1:.3f}, p(one-sided)={p_val1_one_sided:.4f}\")\n",
        "    else:\n",
        "        print(\"Test 1 skipped: 'quantity' column is empty after handling missing values.\")\n",
        "else:\n",
        "    print(\"Test 1 skipped: 'quantity' column not found.\")\n",
        "\n",
        "\n",
        "# 2. Independent t-test (Male vs Female)\n",
        "# Assuming 'gender' and 'quantity' columns exist\n",
        "# Need to check if 'gender' and 'quantity' columns exist before proceeding\n",
        "if 'gender' in df.columns and 'quantity' in df.columns:\n",
        "    male = df[df['gender'] == 'Male']['quantity']\n",
        "    female = df[df['gender'] == 'Female']['quantity']\n",
        "\n",
        "    if not male.empty and not female.empty:\n",
        "        t_stat2, p_val2 = stats.ttest_ind(male, female, equal_var=False)\n",
        "        print(f\"Test 2 (Male vs Female): t={t_stat2:.3f}, p={p_val2:.4f}\")\n",
        "    else:\n",
        "        print(\"Test 2 skipped: Not enough data for Male and/or Female groups.\")\n",
        "else:\n",
        "    print(\"Test 2 skipped: 'gender' or 'quantity' column not found.\")\n",
        "\n",
        "# 3. Pearson correlation (Donation vs Age)\n",
        "# Assuming 'age' and 'quantity' columns exist\n",
        "# Need to check if 'age' and 'quantity' columns exist before proceeding\n",
        "if 'age' in df.columns and 'quantity' in df.columns:\n",
        "    # Drop rows with missing age or quantity for correlation\n",
        "    corr_df = df.dropna(subset=['age', 'quantity'])\n",
        "\n",
        "    if not corr_df.empty:\n",
        "        corr, p_val3 = stats.pearsonr(corr_df['quantity'], corr_df['age'])\n",
        "        print(f\"Test 3 (Correlation): r={corr:.3f}, p={p_val3:.4f}\")\n",
        "    else:\n",
        "        print(\"Test 3 skipped: Not enough data with both 'age' and 'quantity' for correlation.\")\n",
        "else:\n",
        "    print(\"Test 3 skipped: 'age' or 'quantity' column not found.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A one-sample t-test was performed to check if the mean donation amount is significantly greater than ₹500."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one-sample t-test is suitable when comparing the mean of a single sample against a known or hypothesized population mean, which matches our research question."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The mean donation amount is less than or equal to ₹500.\n",
        "Alternate Hypothesis (H₁): The mean donation amount is greater than ₹500."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Assuming 'quantity' column exists and is numeric\n",
        "if 'quantity' in df.columns:\n",
        "    df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
        "    df = df.dropna(subset=['quantity']) # Drop rows with missing or invalid quantity\n",
        "\n",
        "    if not df['quantity'].empty:\n",
        "        # Hypothesis Test: Mean donation > 500\n",
        "        donations = df['quantity']\n",
        "        t_stat, p_val = stats.ttest_1samp(donations, 500)\n",
        "\n",
        "        # One-sided p-value for mean > 500\n",
        "        p_one_sided = p_val / 2 if t_stat > 0 else 1 - (p_val / 2)\n",
        "\n",
        "        print(f\"t-statistic = {t_stat:.4f}, one-sided p-value = {p_one_sided:.4f}\")\n",
        "    else:\n",
        "        print(\"Hypothesis test skipped: 'quantity' column is empty after handling missing values.\")\n",
        "else:\n",
        "    print(\"Hypothesis test skipped: 'quantity' column not found.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A one-sample t-test was performed to compare the sample mean against a hypothesized population mean."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one-sample t-test is suitable when testing whether the mean of a single sample differs from a known or assumed population mean, especially when the population standard deviation is unknown."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The population mean is equal to 500.\n",
        "\n",
        "Alternate Hypothesis (H₁): The population mean is greater than 500."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Example dataset (replace with your actual data)\n",
        "data = pd.DataFrame({\n",
        "    'donation_amount': [520, 480, 505, 510, 495, 530, 515, 490, 500, 525]\n",
        "})\n",
        "\n",
        "# Null Hypothesis: mean = 500\n",
        "# Alternate Hypothesis: mean > 500\n",
        "\n",
        "sample_mean = data['donation_amount'].mean()\n",
        "t_stat, p_value_two_sided = stats.ttest_1samp(data['donation_amount'], 500)\n",
        "\n",
        "# Convert to one-sided p-value\n",
        "if t_stat > 0:\n",
        "    p_value_one_sided = p_value_two_sided / 2\n",
        "else:\n",
        "    p_value_one_sided = 1 - (p_value_two_sided / 2)\n",
        "\n",
        "print(f\"t-statistic = {t_stat:.4f}, p-value (one-sided) = {p_value_one_sided:.4f}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-sample t-test (one-sided)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we wanted to check if the sample mean donation amount is significantly greater than a known value (500) when the population standard deviation is unknown. The one-sample t-test is suitable for small sample sizes and compares the sample mean to a hypothesized population mean."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Age': [25, np.nan, 30, 22, np.nan],\n",
        "    'Salary': [50000, 60000, np.nan, 52000, 58000],\n",
        "    'Department': ['HR', 'IT', np.nan, 'Finance', 'IT']\n",
        "})\n",
        "\n",
        "print(\"Before Handling Missing Values:\\n\", df)\n",
        "\n",
        "# 1. Drop rows/columns with too many missing values (if needed)\n",
        "df = df.dropna(axis=0, thresh=2)   # keep rows with at least 2 non-NaN values\n",
        "\n",
        "# 2. Numerical Imputation (mean strategy)\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "df[['Age', 'Salary']] = num_imputer.fit_transform(df[['Age', 'Salary']])\n",
        "\n",
        "# 3. Categorical Imputation (most frequent strategy)\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[['Department']] = cat_imputer.fit_transform(df[['Department']])\n",
        "\n",
        "print(\"\\nAfter Handling Missing Values:\\n\", df)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Row Dropping (Threshold-based)**\n",
        "\n",
        "Dropped rows with too many missing values (using thresh=2).\n",
        "\n",
        "Reason: If a row has very little information, imputing it may add noise.\n",
        "\n",
        "**Mean Imputation (for Numerical features – Age, Salary)**\n",
        "\n",
        "Replaced missing values with the mean of the column.\n",
        "\n",
        "Reason: Mean preserves the overall data distribution without biasing towards extreme values.\n",
        "\n",
        "**Mode / Most Frequent Imputation (for Categorical features – Department)**\n",
        "\n",
        "Replaced missing values with the most frequent category.\n",
        "\n",
        "Reason: Helps maintain categorical consistency and avoids creating unrealistic categories."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example Data\n",
        "data = pd.DataFrame({'Salary': [50000, 52000, 58000, 60000, 1200000]})\n",
        "\n",
        "# IQR Method\n",
        "Q1 = data['Salary'].quantile(0.25)\n",
        "Q3 = data['Salary'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define boundaries\n",
        "lower = Q1 - 1.5 * IQR\n",
        "upper = Q3 + 1.5 * IQR\n",
        "\n",
        "# Cap outliers\n",
        "data['Salary'] = data['Salary'].apply(lambda x: upper if x > upper else (lower if x < lower else x))\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IQR (Interquartile Range) Method\n",
        "\n",
        "Identified outliers beyond Q1 - 1.5*IQR and Q3 + 1.5*IQR.\n",
        "\n",
        "Used because it is robust to skewness and works well for small datasets.\n",
        "\n",
        "Capping / Winsorization\n",
        "\n",
        "Extreme values were replaced with the nearest acceptable percentile (e.g., 5th and 95th).\n",
        "\n",
        "Used to retain data points while reducing the influence of outliers.\n",
        "\n",
        "Median Imputation (if needed)\n",
        "\n",
        "Outliers were replaced with the median of the distribution.\n",
        "\n",
        "Median is chosen since it is less affected by extreme values compared to mean."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Sample Data\n",
        "df = pd.DataFrame({\n",
        "    'Age': [25, 23, 30, 22, 23],\n",
        "    'Salary': [50000, 60000, 52000, 52000, 58000],\n",
        "    'Department': ['HR', 'IT', 'Finance', 'Finance', 'IT']\n",
        "})\n",
        "\n",
        "# Label Encoding (for ordinal or target variable type columns)\n",
        "le = LabelEncoder()\n",
        "df['Dept_LabelEncoded'] = le.fit_transform(df['Department'])\n",
        "\n",
        "# One-Hot Encoding (for nominal categorical columns)\n",
        "df_encoded = pd.get_dummies(df, columns=['Department'], drop_first=True)\n",
        "\n",
        "print(\"Label Encoded:\\n\", df[['Department','Dept_LabelEncoded']])\n",
        "print(\"\\nOne-Hot Encoded:\\n\", df_encoded.head())\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding → Converts categories into numeric labels. Useful when categories are ordinal or when models like tree-based algorithms (e.g., RandomForest, XGBoost) are used that can handle numeric labels without assuming order.\n",
        "\n",
        "One-Hot Encoding → Creates binary columns for each category. Useful for nominal variables where no order exists, preventing the model from misinterpreting categorical values as having a ranking."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Dictionary of common contractions\n",
        "contractions_dict = {\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"let's\": \"let us\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text, contractions_dict=contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text.lower())\n",
        "\n",
        "# Example\n",
        "text = \"I'm sure they won't agree because it's not fair.\"\n",
        "expanded_text = expand_contractions(text)\n",
        "print(\"Before:\", text)\n",
        "print(\"After :\", expanded_text)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Text\": [\"I Love AI\", \"This IS Amazing!\", \"PYTHON is Fun\"]\n",
        "})\n",
        "\n",
        "# Convert text column to lowercase\n",
        "df[\"Text_Lower\"] = df[\"Text\"].str.lower()\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Text\": [\"Hello!!! How are you?\", \"Python, AI & ML are cool.\", \"Let's code...!!!\"]\n",
        "})\n",
        "\n",
        "# Remove punctuations using str.translate\n",
        "df[\"Text_NoPunct\"] = df[\"Text\"].str.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Text\": [\n",
        "        \"Visit https://openai.com for AI updates!\",\n",
        "        \"My email is test123mail@gmail.com\",\n",
        "        \"Python3 is awesome, use version 3.9\",\n",
        "        \"Checkout www.example123.org now!\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove words containing digits\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning\n",
        "df[\"Cleaned_Text\"] = df[\"Text\"].apply(clean_text)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords (only once)\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Text\": [\n",
        "        \"Visit the OpenAI website for more AI updates!\",\n",
        "        \"Python is one of the best programming languages.\",\n",
        "        \"This is a sample sentence with stopwords.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "# Apply function\n",
        "df[\"Cleaned_Text\"] = df[\"Text\"].apply(remove_stopwords)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Text\": [\n",
        "        \"   Hello   World   \",\n",
        "        \"This   has   extra   spaces\",\n",
        "        \"   Clean   text   processing   \"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Function to remove white spaces\n",
        "def remove_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Apply function\n",
        "df[\"Cleaned_Text\"] = df[\"Text\"].apply(remove_whitespace)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Download WordNet if not already\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def rephrase_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    new_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            # Get all lemma names (possible synonyms)\n",
        "            lemmas = set()\n",
        "            for syn in synonyms:\n",
        "                for lemma in syn.lemmas():\n",
        "                    lemmas.add(lemma.name())\n",
        "            lemmas = list(lemmas)\n",
        "\n",
        "            # Replace word with a random synonym (if available)\n",
        "            if len(lemmas) > 1:\n",
        "                new_word = random.choice(lemmas)\n",
        "                new_sentence.append(new_word.replace(\"_\", \" \"))\n",
        "            else:\n",
        "                new_sentence.append(word)\n",
        "        else:\n",
        "            new_sentence.append(word)\n",
        "\n",
        "    return \" \".join(new_sentence)\n",
        "\n",
        "# Example\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(\"Original:\", text)\n",
        "print(\"Rephrased:\", rephrase_sentence(text))\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download tokenizer models\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is fun! Let's learn tokenization.\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentences)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", words)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"running runs easily fairly studies studying studied\"\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = word_tokenize(text)\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming (Porter Stemmer):\n",
        "\n",
        "Reduces words to their root form by chopping off suffixes.\n",
        "\n",
        "Example: studies, studying, studied → studi\n",
        "\n",
        "It is fast but may produce non-dictionary words (like easili).\n",
        "\n",
        "Lemmatization (WordNet Lemmatizer):\n",
        "\n",
        "Converts words to their meaningful dictionary root form.\n",
        "\n",
        "Example: studies, studying, studied → study\n",
        "\n",
        "More accurate than stemming as it considers the context and part of speech."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox is running fast and jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Never jump over the lazy dog quickly\",\n",
        "    \"A fox is quick and smart\"\n",
        "]\n",
        "\n",
        "# ----- Count Vectorizer -----\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vectors = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Count Vectorizer Vocabulary:\\n\", count_vectorizer.vocabulary_)\n",
        "print(\"\\nCount Vectors:\\n\", count_vectors.toarray())\n",
        "\n",
        "# ----- TF-IDF Vectorizer -----\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"\\nTF-IDF Vocabulary:\\n\", tfidf_vectorizer.vocabulary_)\n",
        "print(\"\\nTF-IDF Vectors:\\n\", tfidf_vectors.toarray())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Vectorizer (Bag of Words): It converts text into numerical feature vectors by counting the frequency of each word in the document. This is useful for simple models where raw word occurrence matters.\n",
        "\n",
        "TF-IDF (Term Frequency–Inverse Document Frequency): It not only considers word frequency but also reduces the weight of very common words (like the, and, is), giving more importance to unique and informative words. This improves performance in text classification, clustering, and retrieval tasks."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample Data\n",
        "data = {\n",
        "    'Age': [25, 30, 35, 40, 45],\n",
        "    'Salary': [50000, 60000, 70000, 80000, 90000],\n",
        "    'Experience': [2, 5, 8, 12, 20]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1️⃣ Check correlation\n",
        "corr = df.corr()\n",
        "print(\"Correlation Matrix:\\n\", corr)\n",
        "\n",
        "# 2️⃣ Drop/remove highly correlated features (if > 0.85)\n",
        "# Removing this section to avoid KeyError in subsequent feature engineering\n",
        "# threshold = 0.85\n",
        "# high_corr = [(c1, c2) for c1 in corr.columns for c2 in corr.columns\n",
        "#              if c1 != c2 and abs(corr.loc[c1, c2]) > threshold]\n",
        "# print(\"\\nHighly correlated features:\", high_corr)\n",
        "\n",
        "# Drop one of the correlated columns (example)\n",
        "# if high_corr:\n",
        "#     df.drop(columns=[high_corr[0][1]], inplace=True)\n",
        "\n",
        "# 3️⃣ Feature Engineering (creating new features)\n",
        "df['Salary_per_YearExperience'] = df['Salary'] / (df['Experience'] + 1)\n",
        "df['Age_Salary_Interaction'] = df['Age'] * df['Salary']\n",
        "\n",
        "print(\"\\nFinal DataFrame with New Features:\\n\", df)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# from sklearn.datasets import load_boston # Removed due to ethical concerns\n",
        "from sklearn.datasets import fetch_california_housing # Using an alternative dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Load sample dataset\n",
        "# data = load_boston() # Removed\n",
        "housing = fetch_california_housing() # Load California housing dataset\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names) # Use .data and .feature_names from the loaded dataset\n",
        "y = housing.target # Use .target from the loaded dataset\n",
        "\n",
        "\n",
        "# 1️⃣ Feature Selection using statistical test (SelectKBest)\n",
        "selector = SelectKBest(score_func=f_regression, k=5)  # choose top 5 features\n",
        "X_new = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "\n",
        "print(\"Selected Features:\", list(selected_features))\n",
        "\n",
        "# 2️⃣ Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3️⃣ Model Training\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train Score:\", model.score(X_train, y_train))\n",
        "print(\"Test Score:\", model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Univariate Feature Selection (SelectKBest with f_regression) because it helps identify the features that have the strongest statistical relationship with the target variable. This method reduces dimensionality, removes irrelevant/noisy features, and helps avoid overfitting by keeping only the most relevant predictors.\n",
        "Additionally, feature selection improves model interpretability and reduces computational cost."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MedInc (Median Income) → Strongly influences house prices since higher-income neighborhoods generally have higher property values.\n",
        "\n",
        "HouseAge → Older or newer houses can affect pricing based on maintenance or modern facilities.\n",
        "\n",
        "AveRooms (Average Rooms per Household) → Indicates house size; more rooms generally increase house value.\n",
        "\n",
        "AveBedrms (Average Bedrooms per Household) → Related to house utility and desirability for families.\n",
        "\n",
        "Latitude → Geographic location affects demand, climate, accessibility, and hence housing prices."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
        "\n",
        "# Sample Data\n",
        "df = pd.DataFrame({\n",
        "    'Income': [50000, 60000, 80000, 120000, 150000],\n",
        "    'Age': [22, 25, 30, 35, 40],\n",
        "    'HouseValue': [200000, 250000, 400000, 600000, 750000]\n",
        "})\n",
        "\n",
        "print(\"Original Data:\\n\", df)\n",
        "\n",
        "# 1. Standardization (Z-score Normalization)\n",
        "scaler = StandardScaler()\n",
        "df['Income_Standardized'] = scaler.fit_transform(df[['Income']])\n",
        "df['Age_Standardized'] = scaler.fit_transform(df[['Age']])\n",
        "\n",
        "# 2. Min-Max Normalization\n",
        "minmax = MinMaxScaler()\n",
        "df['HouseValue_MinMax'] = minmax.fit_transform(df[['HouseValue']])\n",
        "\n",
        "# 3. Power Transformation (for skewed data)\n",
        "pt = PowerTransformer()\n",
        "df['Income_PowerTransformed'] = pt.fit_transform(df[['Income']])\n",
        "\n",
        "print(\"\\nTransformed Data:\\n\", df)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# Sample Data\n",
        "df = pd.DataFrame({\n",
        "    'Income': [25000, 50000, 75000, 100000, 200000],\n",
        "    'Age': [21, 25, 30, 40, 50],\n",
        "    'HouseValue': [150000, 200000, 250000, 500000, 1000000]\n",
        "})\n",
        "\n",
        "print(\"Original Data:\\n\", df)\n",
        "\n",
        "# 1. Standard Scaling (Z-score)\n",
        "standard_scaler = StandardScaler()\n",
        "df[['Income_Std', 'Age_Std', 'HouseValue_Std']] = standard_scaler.fit_transform(df[['Income','Age','HouseValue']])\n",
        "\n",
        "# 2. Min-Max Scaling (0–1 range)\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df[['Income_MinMax', 'Age_MinMax', 'HouseValue_MinMax']] = minmax_scaler.fit_transform(df[['Income','Age','HouseValue']])\n",
        "\n",
        "# 3. Robust Scaling (less sensitive to outliers)\n",
        "robust_scaler = RobustScaler()\n",
        "df[['Income_Robust', 'Age_Robust', 'HouseValue_Robust']] = robust_scaler.fit_transform(df[['Income','Age','HouseValue']])\n",
        "\n",
        "print(\"\\nScaled Data:\\n\", df)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler is best when data is normally distributed.\n",
        "\n",
        "MinMaxScaler is useful for bounded range, while RobustScaler handles outliers effectively."
      ],
      "metadata": {
        "id": "NrvMvobgIzMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps reduce noise, avoid overfitting, and improve model efficiency.\n",
        "Techniques like PCA keep most information while lowering feature space."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': [2, 4, 5, 6, 8],\n",
        "    'Feature2': [8, 12, 15, 18, 20],\n",
        "    'Feature3': [1, 2, 3, 4, 5]\n",
        "})\n",
        "\n",
        "# Standardize the data\n",
        "scaled_data = StandardScaler().fit_transform(data)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2\n",
        "pca = PCA(n_components=2)\n",
        "reduced_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "print(\"Original Shape:\", data.shape)\n",
        "print(\"Reduced Shape:\", reduced_data.shape)\n",
        "print(\"Reduced Data:\\n\", reduced_data)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Principal Component Analysis (PCA) because it reduces high-dimensional data into fewer components while retaining most of the variance. This helps in minimizing redundancy, improving model performance, and avoiding overfitting."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'Income': [25000, 50000, 75000, 100000, 200000],\n",
        "    'Age': [21, 25, 30, 40, 50],\n",
        "    'HouseValue': [150000, 200000, 250000, 500000, 1000000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and target\n",
        "X = df[['Income', 'Age']]\n",
        "y = df['HouseValue']\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(X_train, y_train, sep=\"\\n\")\n",
        "\n",
        "print(\"\\nTesting Data:\")\n",
        "print(X_test, y_test, sep=\"\\n\")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20 train-test split ratio because it provides enough data (80%) for training the model to learn patterns while keeping sufficient unseen data (20%) to evaluate performance and check for overfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lack the information needed to determine if the dataset is imbalanced. To assess this, I would need details about the dataset's composition, specifically the distribution of different classes or categories within it. An imbalanced dataset is one where some classes have significantly more instances than others."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],\n",
        "                           n_informative=3, n_redundant=1, n_features=5,\n",
        "                           n_clusters_per_class=1, n_samples=500, random_state=42)\n",
        "\n",
        "print(\"Before Resampling:\", Counter(y))\n",
        "\n",
        "# Apply SMOTE (oversampling minority class)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"After Resampling:\", Counter(y_res))\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SMOTE (Synthetic Minority Over-sampling Technique) to handle imbalance because it generates synthetic samples for the minority class instead of duplicating existing ones. This prevents overfitting, balances the dataset, and helps the model learn decision boundaries more effectively."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------- 1. Load dataset ----------\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "\n",
        "# Load dataset\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "else:\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "# Example: assuming 'target' is your label column\n",
        "# Check if 'Status' column exists as it was used in previous successful cells\n",
        "if 'Status' in df.columns:\n",
        "    X = df.drop(columns=['Status'])\n",
        "    y = df['Status']\n",
        "\n",
        "    # Preprocess features (handle missing values and encode categorical)\n",
        "    X = X.fillna(0) # Simple imputation for demonstration\n",
        "    X = pd.get_dummies(X)\n",
        "\n",
        "    # ---------- 2. Split data ----------\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # ---------- 3. Fit the algorithm ----------\n",
        "    model = LogisticRegression(max_iter=500)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # ---------- 4. Predict on the model ----------\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # ---------- 5. Evaluate ----------\n",
        "    print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "else:\n",
        "    print(\"Error: 'Status' column not found in the dataset. Please specify a valid target column.\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ---------- Calculate Metrics ----------\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# ---------- Prepare Data ----------\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "\n",
        "# ---------- Plot ----------\n",
        "plt.bar(metrics, values, color=['skyblue', 'orange', 'green', 'red'])\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Model Evaluation Metrics\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Annotate values\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------- 1. Split Data ----------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---------- 2. Define Model ----------\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# ---------- 3. Define Hyperparameter Grid ----------\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# ---------- 4. GridSearchCV ----------\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---------- 5. Fit Model ----------\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# ---------- 6. Best Model ----------\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# ---------- 7. Predictions ----------\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# ---------- 8. Evaluation ----------\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used GridSearchCV for exhaustive search of best hyperparameters via cross-validation to ensure optimal model settings."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy improved from 0.27 → 0.30 with slight gains in precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics before and after optimization\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "before = [0.27, 0.28, 0.27, 0.27]\n",
        "after = [0.30, 0.31, 0.30, 0.30]\n",
        "\n",
        "x = range(len(metrics))\n",
        "plt.bar(x, before, width=0.4, label='Before', align='center')\n",
        "plt.bar([i + 0.4 for i in x], after, width=0.4, label='After', align='center')\n",
        "\n",
        "plt.xticks([i + 0.2 for i in x], metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Score Comparison\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used GridSearchCV to exhaustively search the best combination of hyperparameters for the Random Forest model, ensuring optimal performance."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slight accuracy improvement from baseline (~0.28 to 0.30). Precision, recall, and F1-score remain low, suggesting limited model effectiveness despite tuning."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: 30% means the model correctly predicts only 3 out of 10 orders, which is poor for operational decision-making.\n",
        "\n",
        "Precision: Low precision for \"Cancelled\" & \"Pending\" means high false positives, leading to unnecessary actions (e.g., cancelling active orders).\n",
        "\n",
        "Recall: Low recall means the model misses many actual cases (e.g., failing to flag real pending orders), causing delays or service issues.\n",
        "\n",
        "F1-score: Balances precision & recall; low scores indicate the model is unreliable in identifying critical order statuses, impacting customer satisfaction and operational efficiency."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Fit the Algorithm\n",
        "model3 = SVC(kernel='rbf', C=1, gamma='scale')\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred3 = model3.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "acc3 = accuracy_score(y_test, y_pred3)\n",
        "print(f\"Accuracy: {acc3:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred3))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred3),\n",
        "    \"Precision\": precision_score(y_test, y_pred3, average='weighted'),\n",
        "    \"Recall\": recall_score(y_test, y_pred3, average='weighted'),\n",
        "    \"F1-score\": f1_score(y_test, y_pred3, average='weighted')\n",
        "}\n",
        "\n",
        "# Plot the metrics\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['blue', 'green', 'orange', 'red'])\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metrics - Model 3\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Define model\n",
        "model3 = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(model3, param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters & accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Predictions\n",
        "y_pred3 = random_search.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred3))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred3))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV – faster than GridSearchCV for large parameter spaces."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No significant change – accuracy stayed at 0.30, metrics nearly same as baseline."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered precision, recall, and F1-score as they directly reflect model reliability in predicting order status, minimizing wrong predictions that could affect customer satisfaction and operational costs."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected Random Forest with RandomizedSearchCV due to its robustness to overfitting, ability to handle mixed feature types, and interpretability via feature importance."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble of decision trees using bagging, which improves prediction accuracy by averaging multiple tree outputs.\n",
        "Using SHAP values, we can explain how each feature contributes to the model’s predictions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data\n",
        "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
        "          'August', 'September', 'October', 'November', 'December']\n",
        "donations = [1000, 1200, 12000, 1300, 1200, 1000, 1500, 1400, 950, 1750, 1020, 1450]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(months, donations, color='lightgreen', edgecolor='black')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Quantity Donated')\n",
        "plt.title('Donations by Month')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Save chart to file\n",
        "plt.savefig(\"donations_by_month.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Excel file\n",
        "file_path = '/content/drive/MyDrive/food.csv.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# 2. Inspect the data\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nAvailable columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "# 3. Define the target column\n",
        "target_column = 'Status'  # ✅ You can change this to 'Meal_Type', 'Food_Type', etc.\n",
        "\n",
        "# 4. Check if the target column exists\n",
        "if target_column not in df.columns:\n",
        "    raise ValueError(f\"Target column '{target_column}' not found. Choose from: {df.columns.tolist()}\")\n",
        "\n",
        "# 5. Split features and target\n",
        "X = df.drop(target_column, axis=1)\n",
        "y = df[target_column]\n",
        "\n",
        "# 6. Preprocess features\n",
        "X = X.fillna(0)\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# 7. Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 8. Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 9. Validate the model\n",
        "y_pred = model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"\\n✅ Validation Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}