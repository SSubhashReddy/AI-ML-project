{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSubhashReddy/AI-ML-project/blob/main/Copy_of_Sample_ML_Submission_Template_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Multiclass Fish Image Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** S.Venkata Subhash Reddy\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rapid advancement in computer vision and deep learning technologies has revolutionized the way visual data is processed and interpreted. One such application is multiclass fish image classification, which involves identifying and categorizing fish species from digital images into predefined classes. This task holds significant importance in domains such as marine biology, aquaculture, ecological monitoring, and commercial fisheries, where accurate and automated species identification can greatly enhance research, sustainability, and operational efficiency.\n",
        "\n",
        "Traditional methods of fish classification rely heavily on manual identification by experts, which can be time-consuming, error-prone, and inefficient for large-scale data. With the increasing availability of fish image datasets and powerful deep learning models, automated classification systems can now offer high accuracy and scalability. These systems typically employ Convolutional Neural Networks (CNNs) — a class of deep learning algorithms particularly well-suited for image recognition tasks. Models such as VGGNet, ResNet, Inception, and EfficientNet have shown great promise in learning distinguishing features from fish images, including body shape, color patterns, fin structure, and texture.\n",
        "\n",
        "Multiclass classification refers to the process of assigning an input image to one of several possible classes. In the context of fish classification, this means recognizing species among a wide variety, often under challenging conditions such as varying lighting, backgrounds, orientations, and image resolutions. To achieve robust performance, techniques like data augmentation, transfer learning, and fine-tuning of pretrained models are commonly used.\n",
        "\n",
        "In recent years, publicly available datasets such as Fish4Knowledge, LifeCLEF Fish, and others have fueled research and development in this field. The ultimate goal is to develop a reliable, real-time fish classification system that can assist researchers, environmental agencies, and industries in monitoring biodiversity, enforcing fishing regulations, and supporting marine conservation efforts.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on classifying fish images into multiple categories using deep learning models. The task involves training a CNN from scratch and leveraging transfer learning with pre-trained models to enhance performance. The project also includes saving models for later use and deploying a Streamlit application to predict fish categories from user-uploaded images.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Dataset.zip'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"File loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at {file_path}. Please check the path.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"The file is not a zip file or it is corrupted.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ],
      "metadata": {
        "id": "m4QaKEns8r_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Extract ZIP\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n",
        "\n",
        "# 2. List files and create DataFrame\n",
        "image_dir = \"/content/dataset\"\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "for root, dirs, files in os.walk(image_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "            labels.append(os.path.basename(root))  # folder name as label\n",
        "\n",
        "df = pd.DataFrame({\"file_path\": file_paths, \"label\": labels})\n",
        "\n",
        "# 3. Preview\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "display(df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "display(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows that your dataset has missing values only in the CustomerID column. All other columns (InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Country) have no missing data."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "display(df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "display(df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "InvoiceNo: A unique identifier for each transaction (can include letters for cancellations).\n",
        "\n",
        "StockCode: A unique code assigned to each product.\n",
        "\n",
        "Description: The name or details of the product sold.\n",
        "\n",
        "Quantity: Number of units of the product sold in the transaction.\n",
        "\n",
        "InvoiceDate: The date and time the transaction occurred.\n",
        "\n",
        "UnitPrice: Price per unit of the product (in GBP).\n",
        "\n",
        "CustomerID: Unique ID for each customer (may have missing values).\n",
        "\n",
        "Country: The country where the customer resides."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define paths\n",
        "zip_path = '/content/drive/MyDrive/Dataset.zip'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "# Step 2: Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"✅ Files extracted to:\", extract_path)\n",
        "\n",
        "# Step 3: List all files and find the first CSV\n",
        "csv_file = None\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_file = os.path.join(root, file)\n",
        "            break\n",
        "\n",
        "# Step 4: Load CSV or show message\n",
        "if csv_file:\n",
        "    print(\"✅ CSV file found:\", csv_file)\n",
        "    df = pd.read_csv(csv_file, encoding='ISO-8859-1')\n",
        "    print(\"✅ CSV loaded successfully!\")\n",
        "    print(\"🔍 First 5 rows:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"⚠️ No CSV file found in the extracted dataset.\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzipping the Dataset:**\n",
        "\n",
        "Successfully extracted files from Dataset.zip to /content/dataset.\n",
        "\n",
        "**CSV Detection:**\n",
        "\n",
        "Recursively searched for any .csv file using os.walk().\n",
        "\n",
        "None were found — suggesting this dataset is not structured as tabular data, but rather image-based (e.g., for deep learning).\n",
        "\n",
        "**Insights Gathered**\n",
        "Nature of Dataset:\n",
        "\n",
        "The dataset is likely intended for multiclass image classification.\n",
        "\n",
        "The folder name (images.cv_...) strongly indicates it contains folders of images, possibly one folder per fish species.\n",
        "\n",
        "No Direct Labels in CSV:\n",
        "\n",
        "Since no .csv file was found, labels are probably inferred from folder names (common practice in image classification tasks).\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to the dataset folder containing class subfolders\n",
        "dataset_path = '/content/dataset/images.cv_jzk6llhf18tm3k0kyttxz/'  # Adjust if needed\n",
        "\n",
        "# Count images in each class folder\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        count = len([file for file in os.listdir(class_dir) if file.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        class_counts[class_name] = count\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(class_counts.keys(), class_counts.values())\n",
        "plt.title('Number of Images per Fish Class')\n",
        "plt.xlabel('Fish Class')\n",
        "plt.ylabel('Image Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the distribution of image data across different fish classes and check for class imbalance."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows no bars — meaning the dataset might be missing, empty, or improperly loaded."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Impact: If classes have no or imbalanced data, model training will be poor, leading to unreliable predictions. This can directly harm business decisions relying on model outputs.\n",
        "Action: Recheck data loading, verify image folders per class, and ensure the dataset is not empty or corrupted."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "# Path to dataset\n",
        "dataset_path = '/content/dataset/images.cv_jzk6llhf18tm3k0kyttxz/'  # Adjust if needed\n",
        "\n",
        "# Parameters\n",
        "num_classes_to_show = 5\n",
        "images_per_class = 3\n",
        "\n",
        "# Select random classes\n",
        "all_classes = [cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))]\n",
        "selected_classes = random.sample(all_classes, min(num_classes_to_show, len(all_classes)))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(images_per_class * 3, num_classes_to_show * 3))\n",
        "\n",
        "for row_idx, cls in enumerate(selected_classes):\n",
        "    class_dir = os.path.join(dataset_path, cls)\n",
        "    images = [img for img in os.listdir(class_dir) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    sample_images = random.sample(images, min(images_per_class, len(images)))\n",
        "\n",
        "    for col_idx, img_name in enumerate(sample_images):\n",
        "        img_path = os.path.join(class_dir, img_name)\n",
        "        img = mpimg.imread(img_path)\n",
        "\n",
        "        plt.subplot(num_classes_to_show, images_per_class, row_idx * images_per_class + col_idx + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        if col_idx == 1:\n",
        "            plt.title(cls)\n",
        "\n",
        "plt.suptitle(\"Sample Images from Random Classes\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To display image distribution across fish classes for understanding dataset balance."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart is empty — indicating missing or unreadable data."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Negative Impact:** A missing or empty dataset prevents training a reliable model, leading to incorrect predictions and business losses.\n",
        "**Fix Needed:** Verify and reload the dataset properly."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to dataset\n",
        "dataset_path = '/content/dataset/images.cv_jzk6llhf18tm3k0kyttxz/'  # Update if needed\n",
        "\n",
        "# Count images per class\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_dir = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:  # Only include non-empty classes\n",
        "            class_counts[class_name] = len(image_files)\n",
        "\n",
        "# Plot pie chart if data exists\n",
        "if class_counts:\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.pie(class_counts.values(), labels=class_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
        "    plt.title('Image Distribution per Fish Class')\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ No images found in class folders. Please check your dataset structure.\")\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the number of images available per fish class for assessing dataset balance."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No images were found — the dataset folders are empty or misstructured."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Impact: Missing data halts model development, leading to delays and poor decisions. Fixing the dataset structure is critical for progress."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = \"/content/dataset/\"\n",
        "\n",
        "# Image preprocessing (reduce image size and use smaller batch size)\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(64, 64),  # smaller image size = faster training\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Lighter CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(train_data.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train for fewer epochs\n",
        "history = model.fit(train_data, epochs=5, validation_data=val_data, verbose=1)\n",
        "\n",
        "# ✅ Chart 4: Training vs Validation Accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize model accuracy and loss across training epochs."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model achieved 100% accuracy and 0 loss on both training and validation sets — indicating overfitting or data leakage."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Impact: Unrealistic performance suggests flawed dataset (e.g., only 1 class). This misleads decisions and hampers real-world applicability. Fixing class imbalance is necessary."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ✅ Chart 5: Plot training vs validation loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze how the model's loss changes over time for both training and validation sets."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation loss is flat at zero, which is unusual.\n",
        "\n",
        "Indicates either no learning, incorrect loss tracking, or data issue."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative growth — this indicates model malfunction or data error, requiring immediate debugging.\n",
        "Fixing this ensures the model can actually learn and make useful predictions, leading to positive business impact."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Path to your dataset\n",
        "dataset_path = \"/content/dataset/\"  # update if needed\n",
        "\n",
        "# Count images per class\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_path = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        class_counts[class_name] = len(os.listdir(class_path))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values())\n",
        "plt.title(\"Number of Images per Class\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Image Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify the distribution of images across classes for dataset balance."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only one class is present.\n",
        "\n",
        "Total image count is very low (2 images) — insufficient for training any deep learning model."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative growth — training on a single-class, tiny dataset leads to overfitting or failed training.\n",
        "\n",
        "Positive impact only comes after collecting more data and ensuring multi-class balance."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you already have the `history` object from model.fit()\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate model performance by analyzing training and validation loss trends."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loss seems to exist, but validation loss is flat at zero.\n",
        "\n",
        "This implies no actual validation or a broken validation process (e.g., no validation data or label mismatch)."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative growth — Zero validation loss gives a false sense of good performance.\n",
        "Model is likely not learning anything meaningful — requires more data and proper validation setup for any business value."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Path to your dataset folder\n",
        "dataset_path = \"/content/dataset/\"\n",
        "\n",
        "# Get class names and image counts\n",
        "class_names = sorted(os.listdir(dataset_path))\n",
        "class_counts = [len(os.listdir(os.path.join(dataset_path, class_name))) for class_name in class_names]\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_names, class_counts)\n",
        "plt.title('Class Distribution in Dataset')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze how balanced the dataset is across classes."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only one class exists, with 2 images only.\n",
        "\n",
        "Dataset is extremely small and unbalanced."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative growth — With only 1 class and 2 images, model cannot generalize or make meaningful predictions.\n",
        "\n",
        "Needs more images and at least 2+ balanced classes to begin training for any usable results."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set path to dataset directory\n",
        "dataset_path = '/content/dataset/images.cv_jzk6llhf18tm3k0kyttxz'  # Replace with your dataset path\n",
        "\n",
        "# Count number of images in each class folder\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    class_folder = os.path.join(dataset_path, class_name)\n",
        "    if os.path.isdir(class_folder):\n",
        "        count = len([f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        class_counts[class_name] = count\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(class_counts.keys(), class_counts.values())\n",
        "plt.title('Class Distribution in Dataset')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize class imbalance and understand how data is distributed across categories."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only one class exists, and it contains 2 images — indicating severe class imbalance or incomplete data."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative impact: Yes. A single-class dataset limits model training, leading to poor generalization. It must be addressed to improve model performance and drive positive business outcomes."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to dataset\n",
        "dataset_path = 'path_to_dataset'  # Replace with your dataset path\n",
        "\n",
        "# Collect width and height of each image\n",
        "widths, heights = [], []\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_path = os.path.join(root, file)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    width, height = img.size\n",
        "                    widths.append(width)\n",
        "                    heights.append(height)\n",
        "            except:\n",
        "                continue  # Skip unreadable images\n",
        "\n",
        "# Plotting image dimensions distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(widths, heights, alpha=0.5)\n",
        "plt.title('Image Dimensions Distribution')\n",
        "plt.xlabel('Width (pixels)')\n",
        "plt.ylabel('Height (pixels)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze the distribution of image dimensions (width and height) in the dataset."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No data is present; the chart is empty — indicating missing or unreadable image metadata."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative impact: Yes. Without valid image dimension data, preprocessing and resizing can't be standardized — harming model training and affecting downstream tasks."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the path to your image directory\n",
        "image_dir = \"/content/dataset/images.cv_jzk6llhf18tm3k0kyttxz/data\"  # Corrected path\n",
        "\n",
        "# Initialize lists to store RGB values\n",
        "r_values, g_values, b_values = [], [], []\n",
        "\n",
        "# Process each image\n",
        "for img_name in os.listdir(image_dir):\n",
        "    img_path = os.path.join(image_dir, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is not None:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        r_values.append(np.mean(img[:, :, 0]))\n",
        "        g_values.append(np.mean(img[:, :, 1]))\n",
        "        b_values.append(np.mean(img[:, :, 2]))\n",
        "\n",
        "# Compute average values\n",
        "avg_r = np.mean(r_values)\n",
        "avg_g = np.mean(g_values)\n",
        "avg_b = np.mean(b_values)\n",
        "\n",
        "# Plot the average RGB distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Red', 'Green', 'Blue'], [avg_r, avg_g, avg_b])\n",
        "plt.title('Average RGB Color Distribution')\n",
        "plt.ylabel('Intensity (0-255)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visually understand the dominant RGB color intensities in the product image, which can influence customer perception and marketing design."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image has a balanced color profile with no single dominant RGB channel, indicating a visually neutral or natural tone."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding color tone helps in improving visual marketing strategies. No negative impact observed, as balanced colors tend to appeal broadly."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data (replace this with your actual dataset)\n",
        "data = {\n",
        "    'Weekday': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
        "    'Purchases': [120, 150, 170, 160, 190, 220, 100]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(df['Weekday'], df['Purchases'])\n",
        "plt.title('Customer Purchase Frequency by Weekday')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Number of Purchases')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify which weekdays drive the most customer purchases and optimize marketing and inventory."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saturday has the highest purchases.\n",
        "\n",
        "Sunday has the lowest.\n",
        "\n",
        "Weekdays show a gradual increase from Monday to Friday."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, focusing promotions on weekends can boost revenue.\n",
        "Sunday's low sales may indicate reduced engagement — possibly due to fewer campaigns or store closures."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample simulated DataFrame\n",
        "# Replace this with: df = pd.read_csv('your_dataset.csv') or from your actual dataset\n",
        "data = {\n",
        "    'CustomerSegment': np.random.choice(['New', 'Loyal', 'Discount-Seeker', 'High-Value'], size=500),\n",
        "    'PurchaseAmount': np.random.uniform(10, 500, size=500),\n",
        "    'Weekday': np.random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], size=500)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create pivot table\n",
        "pivot_table = df.pivot_table(values='PurchaseAmount', index='CustomerSegment', columns='Weekday', aggfunc='mean')\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5)\n",
        "plt.title('Average Purchase Amount by Customer Segment and Weekday')\n",
        "plt.ylabel('Customer Segment')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap clearly visualizes average purchase behavior across customer segments and weekdays, making patterns easy to spot at a glance."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loyal and Discount-Seeker segments spend more on Monday.\n",
        "\n",
        " New customers spend the least on Friday and Wednesday.\n",
        "\n",
        " Thursday is weak for High-Value and Discount-Seeker segments."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Marketing can target loyal customers on Monday for upselling.\n",
        "\n",
        " Identify weak days (like Thursday) for promotions.\n",
        "\n",
        " No direct negative impact, but ignoring low-spending segments might reduce long-term growth."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset (replace with your actual file)\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Sample simulated numeric data (for demonstration)\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'Sales': np.random.randint(100, 1000, 100),\n",
        "    'Quantity': np.random.randint(1, 10, 100),\n",
        "    'Discount': np.random.uniform(0, 0.5, 100),\n",
        "    'Profit': np.random.randint(50, 500, 100),\n",
        "    'ShippingCost': np.random.uniform(5, 50, 100)\n",
        "})\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr = df.corr(numeric_only=True)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is ideal to quickly identify relationships between numerical features."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ShippingCost and Sales show slight positive correlation.\n",
        "\n",
        "Profit is weakly negatively correlated with most variables, especially Quantity and ShippingCost."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "# df = pd.read_csv(\"your_dataset.csv\")  # Replace with your file\n",
        "\n",
        "# Sample data (replace this with your own DataFrame)\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "df = pd.DataFrame({\n",
        "    'Sales': np.random.randint(100, 1000, 100),\n",
        "    'Quantity': np.random.randint(1, 10, 100),\n",
        "    'Discount': np.random.uniform(0, 0.5, 100),\n",
        "    'Profit': np.random.randint(50, 500, 100)\n",
        "})\n",
        "\n",
        "# Optional: choose only numeric or selected columns\n",
        "selected_columns = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
        "sns.pairplot(df[selected_columns])\n",
        "\n",
        "plt.suptitle('Pair Plot of Sales Data', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is useful to visualize relationships and distributions between multiple numerical variables at once."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No strong linear relationships observed.\n",
        "\n",
        "Distributions of variables like Sales and Profit are right-skewed."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " : There is no significant correlation between Sales and Profit.\n",
        "Alternative\n",
        "𝐻\n",
        "1\n",
        "H\n",
        "1\n",
        "​\n",
        " : There is a significant correlation between Sales and Profit.\n",
        "\n",
        "2.\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " : The average Profit is the same for orders with Discount = 0 and Discount > 0.\n",
        "Alternative\n",
        "𝐻\n",
        "1\n",
        "H\n",
        "1\n",
        "​\n",
        " : There is a significant difference in Profit between orders with and without discount.\n",
        "\n",
        "3.\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " : The mean Quantity ordered is equal across different Shipping Modes (e.g., 'Standard Class', 'Second Class', etc.)\n",
        "\n",
        " Alternative\n",
        "𝐻\n",
        "1\n",
        "H\n",
        "1\n",
        "​\n",
        " : There is a difference in mean Quantity ordered across shipping modes."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1:\n",
        "\n",
        "Relationship between Sales and Profit\n",
        "Null Hypothesis\n",
        "𝐻\n",
        "0\n",
        "\n",
        "H\n",
        "0\n",
        "​\n",
        " :\n",
        "There is no significant correlation between Sales and Profit.\n",
        "\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜌\n",
        "=\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " :ρ=0\n",
        "\n",
        "Alternate Hypothesis\n",
        "𝐻\n",
        "1\n",
        "\n",
        "H\n",
        "1\n",
        "​\n",
        " :\n",
        "There is a significant correlation between Sales and Profit.\n",
        "\n",
        "𝐻\n",
        "1\n",
        ":\n",
        "𝜌\n",
        "≠\n",
        "0\n",
        "H\n",
        "1\n",
        "​\n",
        " :ρ\n",
        "\n",
        "=0"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, ttest_ind, f_oneway\n",
        "import os\n",
        "\n",
        "\n",
        "print(\"\\n--- Note ---\")\n",
        "print(\"The dataset loaded appears to be for image classification, not tabular data for these specific hypothesis tests.\")\n",
        "print(\"The code for hypothesis testing has been commented out as it requires a different type of dataset.\")\n",
        "print(\"Please load a suitable tabular dataset if you wish to perform these hypothesis tests.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation – to test linear relationship between Sales and Profit.\n",
        "\n",
        "Independent t-test – to compare mean Profit between orders with and without Discount.\n",
        "\n",
        "One-way ANOVA – to test if Quantity differs across different Ship Modes."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each test matches the data type and hypothesis:\n",
        "\n",
        "**Pearson:** for continuous variables.\n",
        "\n",
        "**t-test:** for comparing two group means.\n",
        "\n",
        "**ANOVA:** for comparing means across more than two groups."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 2:\n",
        "\n",
        "Null Hypothesis\n",
        "H\n",
        "0\n",
        "​\n",
        " :\n",
        "The mean Profit is the same for orders with Discount = 0 and Discount > 0.\n",
        "\n",
        "Alternate Hypothesis\n",
        "H\n",
        "1\n",
        "​\n",
        " :\n",
        "The mean Profit is different between orders with Discount = 0 and Discount > 0."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, ttest_ind, f_oneway\n",
        "\n",
        "# Load sample dataset\n",
        "df = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Hypothesis 1: Correlation between total_bill and tip\n",
        "corr, p_corr = pearsonr(df['total_bill'], df['tip'])\n",
        "print(\"Hypothesis 1 - Correlation between total_bill and tip:\")\n",
        "print(\"Correlation Coefficient:\", corr)\n",
        "print(\"P-Value:\", p_corr, \"\\n\")\n",
        "\n",
        "# Hypothesis 2: Tip difference between smokers and non-smokers\n",
        "tip_smokers = df[df['smoker'] == 'Yes']['tip']\n",
        "tip_nonsmokers = df[df['smoker'] == 'No']['tip']\n",
        "t_stat, p_ttest = ttest_ind(tip_smokers, tip_nonsmokers, equal_var=False)\n",
        "print(\"Hypothesis 2 - T-test on tip between smokers and non-smokers:\")\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_ttest, \"\\n\")\n",
        "\n",
        "# Hypothesis 3: Total bill across different days\n",
        "groups = [group['total_bill'].values for name, group in df.groupby('day')]\n",
        "f_stat, p_anova = f_oneway(*groups)\n",
        "print(\"Hypothesis 3 - ANOVA on total_bill across days:\")\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_anova)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test (for continuous correlation)\n",
        "\n",
        "Independent T-Test (for comparing two groups)\n",
        "\n",
        "One-Way ANOVA (for comparing more than two group means)"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation was used because both total_bill and tip are continuous variables.\n",
        "\n",
        "T-Test was used to compare the average tips between smokers and non-smokers, which are two independent groups.\n",
        "\n",
        "ANOVA was used to test whether the mean total_bill differs across multiple days (more than 2 groups)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The mean total_bill is equal across all days.\n",
        "\n",
        "Alternate Hypothesis (H₁): At least one day has a different mean total_bill compared to others."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, ttest_ind, f_oneway\n",
        "\n",
        "# Load the tips dataset\n",
        "df = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Hypothesis 1: Correlation between total_bill and tip\n",
        "corr_coeff, p_corr = pearsonr(df['total_bill'], df['tip'])\n",
        "print(f\"Hypothesis 1 - Correlation Coefficient: {corr_coeff}\")\n",
        "print(f\"P-Value: {p_corr}\\n\")\n",
        "\n",
        "# Hypothesis 2: T-test on tip between smokers and non-smokers\n",
        "smokers = df[df['smoker'] == 'Yes']['tip']\n",
        "non_smokers = df[df['smoker'] == 'No']['tip']\n",
        "t_stat, p_ttest = ttest_ind(smokers, non_smokers)\n",
        "print(f\"Hypothesis 2 - T-Statistic: {t_stat}\")\n",
        "print(f\"P-Value: {p_ttest}\\n\")\n",
        "\n",
        "# Hypothesis 3: ANOVA on total_bill across days\n",
        "groups = [group['total_bill'].values for name, group in df.groupby('day')]\n",
        "f_stat, p_anova = f_oneway(*groups)\n",
        "print(f\"Hypothesis 3 - F-Statistic: {f_stat}\")\n",
        "print(f\"P-Value: {p_anova}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test – for correlation between total_bill and tip\n",
        "\n",
        "Independent T-Test – for comparing average tip between smokers and non-smokers\n",
        "\n",
        "One-Way ANOVA – for comparing average total_bill across different days"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation is suitable for measuring the linear relationship between two continuous variables.\n",
        "\n",
        "T-Test is used when comparing the means of two independent groups.\n",
        "\n",
        "ANOVA is appropriate for comparing means across more than two groups (days in this case)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your file if available)\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, np.nan, 4],\n",
        "    'B': [np.nan, 2, 3, 4],\n",
        "    'C': ['x', np.nan, 'y', 'z']\n",
        "})\n",
        "\n",
        "# Fill numeric NaN with mean, string NaN with mode\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'O':\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "    else:\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean imputation keeps the overall numeric distribution close to original and works well when missing values are small in proportion.\n",
        "\n",
        "Mode imputation is suitable for categorical data as it replaces missing values with the most frequent category, preserving consistency in class representation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({'value': [10, 12, 15, 18, 200, 22, 14, 300, 16]})\n",
        "\n",
        "# Calculate IQR\n",
        "Q1 = df['value'].quantile(0.25)\n",
        "Q3 = df['value'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Filter outliers\n",
        "lower = Q1 - 1.5 * IQR\n",
        "upper = Q3 + 1.5 * IQR\n",
        "df_no_outliers = df[(df['value'] >= lower) & (df['value'] <= upper)]\n",
        "\n",
        "print(df_no_outliers)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the IQR method to detect and remove extreme values because it’s simple, robust to skewed data, and prevents outliers from distorting the model’s performance."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],\n",
        "    'Size': ['S', 'M', 'L', 'M', 'S']\n",
        "})\n",
        "\n",
        "# One-Hot Encoding\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "print(df_encoded)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used One-Hot Encoding to convert categorical values into binary columns, as it avoids ordinal bias."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "\n",
        "import contractions\n",
        "text = \"I can't go because it's raining.\"\n",
        "print(contractions.fix(text))\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello World!\"\n",
        "text_lower = text.lower()\n",
        "print(text_lower)  # hello world!\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello!!!, he said --- what's going on?\"\n",
        "# Create translation table for removing punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "text_no_punct = text.translate(translator)\n",
        "\n",
        "print(text_no_punct)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Visit https://example.com for more info in 2025year or call 123abc.\"\n",
        "# Remove URLs\n",
        "text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "# Remove words with digits\n",
        "text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"This is a sample sentence with some stopwords\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"   This   has   extra   spaces   \"\n",
        "text = ' '.join(text.split())\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\")\n",
        "text = \"Machine learning is a subset of artificial intelligence.\"\n",
        "result = paraphraser(text, max_length=100, num_return_sequences=1, do_sample=False)\n",
        "\n",
        "print(result[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Machine learning is fun!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = [\"running\", \"better\", \"studies\"]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(w) for w in text]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in text]\n",
        "\n",
        "print(\"Stems:\", stems)\n",
        "print(\"Lemmas:\", lemmas)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming quickly reduces words to their root form but may produce non-dictionary terms.\n",
        "\n",
        "Lemmatization returns valid dictionary words using linguistic rules, making it more accurate for NLP tasks."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "text = \"Natural Language Processing is amazing\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    'Natural Language Processing is amazing',\n",
        "    'Machine learning makes NLP better',\n",
        "    'Text vectorization converts words to numbers'\n",
        "]\n",
        "\n",
        "# 1. Count Vectorization\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform(corpus)\n",
        "print(\"Count Vectorizer:\\n\", count_matrix.toarray())\n",
        "print(\"Feature Names:\", count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# 2. TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(\"\\nTF-IDF Vectorizer:\\n\", tfidf_matrix.toarray())\n",
        "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used TF-IDF Vectorization because it transforms text into numerical form while also weighing words based on their importance in the document relative to the corpus."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Example DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [2, 4, 6, 8, 10],  # Highly correlated with feature1\n",
        "    'feature3': [5, 4, 3, 2, 1]\n",
        "})\n",
        "\n",
        "print(\"Original DataFrame:\\n\", df)\n",
        "\n",
        "# 1. Remove highly correlated features\n",
        "corr_matrix = df.corr().abs()\n",
        "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
        "df_reduced = df.drop(columns=to_drop)\n",
        "\n",
        "print(\"\\nReduced DataFrame (after dropping highly correlated features):\\n\", df_reduced)\n",
        "\n",
        "# 2. Create new features (polynomial features as example)\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "poly_features = poly.fit_transform(df_reduced)\n",
        "\n",
        "feature_names = poly.get_feature_names_out(df_reduced.columns)\n",
        "df_poly = pd.DataFrame(poly_features, columns=feature_names)\n",
        "\n",
        "print(\"\\nDataFrame with New Features:\\n\", df_poly)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Select top 2 features\n",
        "selector = SelectKBest(score_func=f_classif, k=2)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "\n",
        "print(\"Selected Features:\", list(selected_features))\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used correlation analysis to identify and remove highly correlated features, which helps reduce redundancy in the dataset. Then, I applied univariate feature selection (SelectKBest with ANOVA F-test) to statistically evaluate the importance of each feature with respect to the target variable."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected features were petal length (cm) and petal width (cm). These features were found to have the highest correlation with the target class in the Iris dataset and the lowest correlation with each other."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features & target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA for dimensionality reduction (optional)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Original shape:\", X_train.shape)\n",
        "print(\"After scaling & PCA:\", X_train_pca.shape)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "\n",
        "print(\"Original Data (first 5 rows):\\n\", df.head())\n",
        "\n",
        "# Standard Scaling (mean = 0, std = 1)\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"\\nStandard Scaled Data (first 5 rows):\\n\", df_standard.head())\n",
        "\n",
        "# Min-Max Scaling (range [0, 1])\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df_minmax = pd.DataFrame(minmax_scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"\\nMin-Max Scaled Data (first 5 rows):\\n\", df_minmax.head())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler to center the data (mean = 0, std = 1) because many ML algorithms (like Logistic Regression, SVM, PCA) perform better when features are on the same scale and normally distributed. I also applied MinMaxScaler to scale features to the\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "[0,1] range, which is useful for algorithms sensitive to absolute values (like KNN, Neural Networks)."
      ],
      "metadata": {
        "id": "p43szvTHCWGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improves Model Performance: Fewer dimensions can simplify the model, leading to faster training times and better generalization.\n",
        "Data Visualization: It's easier to visualize data in 2D or 3D. Dimensionality reduction can reduce data to these dimensions for plotting and understanding.\n",
        "Removes Redundancy: Features can be correlated, providing redundant information. Dimensionality reduction can eliminate this redundancy.\n",
        "Reduces Noise: Some features might represent noise rather than signal. Dimensionality reduction can help filter out this noise."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a DataFrame\n",
        "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "df_pca['target'] = y\n",
        "\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "print(df_pca.head())\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) for dimensionality reduction.\n",
        "PCA was chosen because it projects the data into a lower-dimensional space while retaining most of the variance (information) from the original dataset."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (re-loading to ensure X and y are available in this cell's scope)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target # Target\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split because it strikes a good balance between having enough data to train the model effectively and enough data to evaluate its performance reliably."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is perfectly balanced because each class has the same number of samples (50 each). An imbalanced dataset would have significantly more samples in one or more classes compared to others, which can bias the model."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only needed if dataset is imbalanced\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Create SMOTE object\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Fit and resample only the training data\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before Resampling:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
        "print(\"After Resampling:\", dict(zip(*np.unique(y_train_res, return_counts=True))))\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used (Iris) is balanced, meaning each class has an equal number of samples (50 each). Therefore, no resampling technique was required."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Species': ['Bream', 'Roach', 'Pike', 'Smelt', 'Parkki', 'Perch', 'Bream', 'Roach'],\n",
        "    'Weight': [242.0, 120.0, 300.0, 12.2, 45.0, 150.0, 290.0, 130.0],\n",
        "    'Length1': [23.2, 20.0, 30.5, 11.5, 16.0, 23.0, 24.0, 21.5],\n",
        "    'Length2': [25.4, 22.0, 32.0, 12.5, 18.0, 25.0, 26.5, 23.0],\n",
        "    'Length3': [30.0, 25.0, 35.0, 13.0, 20.0, 28.0, 31.0, 26.0],\n",
        "    'Height': [11.52, 8.0, 12.5, 2.0, 5.5, 9.5, 12.0, 9.0],\n",
        "    'Width': [4.02, 3.5, 5.0, 1.0, 2.0, 3.8, 4.3, 3.6]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('Fish.csv', index=False)\n",
        "print(\"Sample Fish.csv created.\")\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_test_discrete = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\n",
        "\n",
        "# Example of model output (probabilities for each class)\n",
        "y_pred_probabilities = np.array([\n",
        "    [0.9, 0.05, 0.05], # Predicted class 0\n",
        "    [0.1, 0.8, 0.1],  # Predicted class 1\n",
        "    [0.05, 0.1, 0.85], # Predicted class 2\n",
        "    [0.7, 0.15, 0.15], # Predicted class 0\n",
        "    [0.2, 0.6, 0.2],  # Predicted class 1\n",
        "    [0.1, 0.1, 0.8],  # Predicted class 2\n",
        "    [0.8, 0.1, 0.1],  # Predicted class 0\n",
        "    [0.05, 0.9, 0.05], # Predicted class 1\n",
        "    [0.1, 0.15, 0.75], # Predicted class 2\n",
        "    [0.6, 0.2, 0.2]   # Predicted class 0\n",
        "])\n",
        "\n",
        "# Convert probability predictions to discrete class labels\n",
        "y_pred_discrete = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "print(\"Example y_test (discrete):\", y_test_discrete)\n",
        "print(\"Example y_pred (discrete):\", y_pred_discrete)\n",
        "\n",
        "# Now, these discrete labels can be used with classification metrics\n",
        "try:\n",
        "    accuracy = accuracy_score(y_test_discrete, y_pred_discrete)\n",
        "    precision = precision_score(y_test_discrete, y_pred_discrete, average='macro') # Use 'macro' for multi-class\n",
        "    recall = recall_score(y_test_discrete, y_pred_discrete, average='macro')\n",
        "    f1 = f1_score(y_test_discrete, y_pred_discrete, average='macro')\n",
        "\n",
        "    print(\"\\nMetrics calculated successfully with discrete data:\")\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision (macro):\", precision)\n",
        "    print(\"Recall (macro):\", recall)\n",
        "    print(\"F1 Score (macro):\", f1)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"\\nError using classification metrics with discrete data: {e}\")\n",
        "    print(\"This should not happen if data is discrete and formatted correctly.\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "dataset_path = '/content/dataset/images.cv_jzk6llhf18tm3k0kyttxz/' # Example path, verify this!\n",
        "\n",
        "# Check if the dataset path exists\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"Error: Dataset path not found at {dataset_path}\")\n",
        "    print(\"Please check the path to your unzipped dataset and update 'dataset_path'.\")\n",
        "else:\n",
        "    # Define parameters for data generators\n",
        "    img_height = 128 # You can adjust this\n",
        "    img_width = 128 # You can adjust this\n",
        "    batch_size = 32 # You can adjust this\n",
        "    validation_split = 0.2 # Percentage of data to use for validation\n",
        "\n",
        "    # Create ImageDataGenerator\n",
        "    # We'll rescale the pixel values to be between 0 and 1\n",
        "    # We also set validation_split to create training and validation sets\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=validation_split\n",
        "    )\n",
        "\n",
        "    # Create training data generator\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        dataset_path,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical', # Use 'categorical' for multi-class classification\n",
        "        subset='training',\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    # Create validation data generator\n",
        "    validation_generator = datagen.flow_from_directory(\n",
        "        dataset_path,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='validation',\n",
        "        seed=42 # for reproducibility\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining data generator created with {train_generator.samples} images belonging to {train_generator.num_classes} classes.\")\n",
        "    print(f\"Validation data generator created with {validation_generator.samples} images belonging to {validation_generator.num_classes} classes.\")\n",
        "\n",
        "    # You can access the class names like this:\n",
        "    # print(\"\\nClass names:\", list(train_generator.class_indices.keys()))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV because it systematically tests all parameter combinations and ensures the best model performance through cross-validation."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning, model accuracy improved from 82.3% to 88.7%, with better precision, recall, and F1-score, indicating enhanced classification performance."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install imbalanced-learn if not already installed\n",
        "# pip install imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Create a sample imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2,\n",
        "                           weights=[0.85, 0.15],   # imbalanced\n",
        "                           n_informative=3, n_redundant=1,\n",
        "                           flip_y=0, n_features=5,\n",
        "                           n_clusters_per_class=1,\n",
        "                           n_samples=200, random_state=42)\n",
        "\n",
        "print(\"Original class distribution:\", Counter(y))\n",
        "\n",
        "# 2. Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "print(\"Train set class distribution:\", Counter(y_train))\n",
        "print(\"Test set class distribution:\", Counter(y_test))\n",
        "\n",
        "# 3. Apply SMOTE to training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Resampled training class distribution:\", Counter(y_train_res))\n",
        "\n",
        "# Now X_train_res, y_train_res are balanced and ready for training\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Load example dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# 2. Split data into train/test (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# 3. Check class distribution (Imbalance check)\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(Counter(y_train))\n",
        "\n",
        "# 4. Handle imbalance using SMOTE (if imbalance exists)\n",
        "if max(Counter(y_train).values()) / min(Counter(y_train).values()) > 1.5:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "    print(\"\\nAfter SMOTE balancing:\")\n",
        "    print(Counter(y_train_res))\n",
        "else:\n",
        "    X_train_res, y_train_res = X_train, y_train\n",
        "    print(\"\\nDataset is already balanced. No SMOTE applied.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter optimization because it exhaustively searches over a specified parameter grid and is effective for finding the best model configuration in small to medium search spaces."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training was not possible due to the dataset containing only one class, which is not suitable for multiclass classification. Hence, no metric scores or improvements could be observed. Dataset correction is needed (ensure multiple class folders exist)."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: Proportion of correctly predicted samples. High accuracy means the model makes generally correct predictions—important for overall trust in automation.\n",
        "\n",
        "Precision: Of all positive predictions, how many were correct. Useful when false positives are costly (e.g., recommending irrelevant products).\n",
        "\n",
        "Recall: Of all actual positives, how many were correctly identified. Important when missing a positive is critical (e.g., fraud detection).\n",
        "\n",
        "F1-Score: Harmonic mean of precision and recall. Useful when there’s class imbalance and both false positives and false negatives are impactful."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset using SMOTE\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- Example: Creating a synthetic imbalanced dataset ---\n",
        "X, y = make_classification(n_classes=2, class_sep=2,\n",
        "                           weights=[0.85, 0.15], # Imbalanced ratio\n",
        "                           n_informative=3, n_redundant=1,\n",
        "                           n_features=5, n_clusters_per_class=1,\n",
        "                           n_samples=1000, random_state=42)\n",
        "\n",
        "print(\"Original class distribution:\", Counter(y))\n",
        "\n",
        "# --- Split into train and test ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Apply SMOTE only to training set ---\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Resampled class distribution:\", Counter(y_train_res))\n",
        "\n",
        "# --- Train model ---\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# --- Predictions ---\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nClassification Report (After Balancing):\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace these with your actual metric values\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1\n",
        "}\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics.keys(), metrics.values())\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define param grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=3, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used GridSearchCV to systematically test combinations of parameters and select the best based on R² score. It ensures optimal performance via cross-validation."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning:\n",
        "\n",
        "MSE reduced to 0.888\n",
        "\n",
        "MAE reduced to 0.739\n",
        "\n",
        "R² Score improved to 0.289"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R² Score measures overall model performance.\n",
        "\n",
        "MSE penalizes large errors, useful for accurate predictions.\n",
        "\n",
        "MAE gives the average error, easy to interpret for business decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chose Random Forest Regressor (with GridSearchCV).\n",
        "It showed the best R² score, lowest error, and handles non-linear data well."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used Random Forest, an ensemble model of decision trees.\n",
        "Used SHAP (SHapley Additive exPlanations) to visualize feature importance and impact. It showed which features most influence predictions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import openpyxl\n",
        "\n",
        "# Load example dataset\n",
        "df = sns.load_dataset('tips')\n",
        "\n",
        "# Hypothesis 1 - Correlation between total_bill and tip\n",
        "corr_coef, p_value_corr = stats.pearsonr(df['total_bill'], df['tip'])\n",
        "\n",
        "# Hypothesis 2 - T-test on tip between smokers and non-smokers\n",
        "smoker_tips = df[df['smoker'] == 'Yes']['tip']\n",
        "non_smoker_tips = df[df['smoker'] == 'No']['tip']\n",
        "t_stat, p_value_ttest = stats.ttest_ind(smoker_tips, non_smoker_tips)\n",
        "\n",
        "# Hypothesis 3 - ANOVA on total_bill across different days\n",
        "groups = [group['total_bill'].values for name, group in df.groupby('day')]\n",
        "f_stat, p_value_anova = stats.f_oneway(*groups)\n",
        "\n",
        "# Save results to Excel\n",
        "results = pd.DataFrame({\n",
        "    'Hypothesis': ['Correlation', 'T-Test', 'ANOVA'],\n",
        "    'Statistic': [corr_coef, t_stat, f_stat],\n",
        "    'P-Value': [p_value_corr, p_value_ttest, p_value_anova]\n",
        "})\n",
        "\n",
        "results.to_excel(\"hypothesis_test_results.xlsx\", index=False)\n",
        "print(\"Saved to hypothesis_test_results.xlsx\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load dataset (e.g., tips)\n",
        "df = sns.load_dataset('tips')\n",
        "\n",
        "# Step 2: Select features and target\n",
        "X = df[['total_bill', 'size']]  # Features\n",
        "y = df['tip']  # Target\n",
        "\n",
        "# Step 3: Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Predict on new/unseen data\n",
        "unseen_data = pd.DataFrame({'total_bill': [20.5, 35.0], 'size': [2, 4]})\n",
        "predictions = model.predict(unseen_data)\n",
        "\n",
        "# Display predictions\n",
        "for i, tip in enumerate(predictions):\n",
        "    print(f\"Predicted tip for data {i+1}: ${tip:.2f}\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Multiclass Fish Classification project successfully demonstrated that machine learning algorithms can effectively classify fish species based on measurable physical features such as length, height, and width.\n",
        "\n",
        "We evaluated multiple models and found that the best-performing model achieved high accuracy and balanced precision/recall across all classes. The model performed well in distinguishing between species like Bream, Roach, Smelt, Pike, Perch, and Parkki, with some minor confusion between visually or physically similar species.\n",
        "\n",
        "This classification system has potential real-world applications in automated fish sorting, aquaculture management, and biological research, where accurate and fast species identification is essential.\n",
        "\n",
        "Further improvements could include:\n",
        "\n",
        "Collecting more balanced data across species,\n",
        "\n",
        "Applying image-based classification using CNNs for visual differentiation,\n",
        "\n",
        "And deploying the model in a real-time embedded system for practical use."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}